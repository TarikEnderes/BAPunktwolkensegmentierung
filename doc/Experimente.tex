\chapter{Experimente}
	In diesem Kapitel sollen die im Rahmen der Arbeit durchgeführten Experimente aufgeführt, sowie deren Ergebnisse dargelegt und diskutiert werden. Ziel der Experimente ist es, Praktikenzu ermitteln, die bei der Anwendung des entwickelten Systems vorteilhaft sind und mit denen die bestmöglichen Ergebnisse produziert werden.\\
	Zunächst soll die Auswirkung des verwendeten Backbones auf die Leistungsfähigkeit des Netzes untersucht werden, wobei sich die Experimente auf MobileNetV2 konzentrieren. Dazu soll zunächst experimentell die ideale Trainingsdauer für die Verwendung von MobileNetV2 ermittelt werden. Anschließend daran wird das so erstellte Model auf einem Evaluierungs-Datensatz ausgewertet. Die Ergebnisse eines Netzes, das Xception65 als Backbone verwendet sollen ebenfalls diskutiert und mit denen von MobileNetV2 bezüglich deren Qualität und notwendiger Rechenzeit verglichen werden.\\
	Als nächstes soll festgestellt werden, wie gut das Netz unter der Verwendung von MobileNetV2 Bilder aus einem anderen Datensatz verarbeitet und wie sich die Ergebnisse durch Anpassung des Trainingsverhaltens verbessern lassen. Als letztes sollen auf die in Abschnitt \ref{sec:algo} beschriebene Weise segmentierte Punktwolken betrachtet und diskutiert werden. Dabei soll vor allem auf den Zusammenhang zwischen der Qualität der segmentierten Bilder und der der Punktwolken geachtet werden.
	\section{Backbones}
		Für die Durchführung folgender Experimente wird der fein annotierte Cityscapes-Datensatz verwendet. Die Netze werden auf dem aus 2975 Bildern bestehenden Trainingssatz trainiert und auf dem 500 Bilder fassenden Validierungssatz ausgewertet.\\
		Für jede Trainingsepoche werden aus dem Trainingssatz 1487 Bilder zufällig ausgewählt. Als Grundlernrate wird 0.007 gewählt. Der Trainingsfehler wird über die in \cite{Lovasz} beschriebene Lovasz-Funktion berechnet. Weitere Hyperparameter sind eine Dropout-Rate von 0.1,  eine L2 Regularisierungsrate von $4*10^{-5}$ und ein Momentum-Faktor von 0.9.\\
		Die Netzwerkausgaben werden mittels der im Bereich Bildsegmentierung üblichen Intersection-over-union-Metrik (IoU) ausgewertet, die sich folgendermaßen berechnet:\\
		\begin{equation}
			IoU = \frac{TP}{TP + FP + FN}
		\end{equation}
		Dabei steht TP für True Positives, also richtig erkannte Pixel, FP für False Positives und FN für False Negatives. Da in diesem System jedem  Pixel eine gültige Klasse zugeordnet wird, sind FP und FN hier stets identisch.
		
		\subsection{MobileNetV2}
			\label{sec:mn2}
			Da Echtzeitfähigkeit eine wichtige Rolle spielt, konzentrieren sich die Experimente auf das leichtgewichtige MobileNetV2, statt dem leistungsfähigeren Xception65. Das erste Experiment beschäftigt sich mit dem Finden der idealen Trainingsdauer.
			%\subsubsection{Auswirkungen der Trainingsdauer}
			\begin{table}[h]
				\centering
				\begin{tabular}{l|cccccc}
					Epochen trainiert & 1 & 5 & 10 & 15 & 25 & 30\\\hline
					IoU Straße & 0.6260 & 0.6498 & 0.6727 & 0.6729 & 0.6774 & 0.6675\\\hline
					IoU Gehsteig & 0.1835 & 0.3788 & 0.4330 & 0.4616 & 0.5193 & 0.5078\\\hline
					IoU Gebäude & 0.5563 & 0.6402 & 0.6573 & 0.6771 & 0.6931 & 0.7068\\\hline
					IoU Mauer & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Zaun & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Pfahl & 0.0791 & 0.1931 & 0.2159 & 0.2561 & 0.2835 & 0.2728\\\hline
					IoU Ampel & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Verkehrszeichen & 0.0 & 0.1056 & 0.1785 & 0.1859 & 0.2233 & 0.2291\\\hline
					IoU Vegetation & 0.5581 & 0.7153 & 0.7273 & 0.7518 & 0.7728 & 0.7663\\\hline
					IoU Gelände & 0.0 & 0.0915 & 0.1274 & 0.1355 & 0.1593 & 0.1445\\\hline
					IoU Himmel & 0.5353 & 0.6606 & 0.6867 & 0.6977 & 0.7153 & 0.7081\\\hline
					IoU Person & 0.0 & 0.1160 & 0.1582 & 0.1458 & 0.1680 & 0.1863\\\hline
					IoU Radfahrer & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Auto & 0.3537 & 0.5438 & 0.6014 & 0.5863& 0.6419 & 0.6103\\\hline
					IoU Lastwagen & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Bus & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Zug & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Motorrad & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Fahrrad & 0.0 & 0.0 & 0.0 & 0.0750 & 0.1340 & 0.1542\\\hline
					Durchschnitt IoU & 0.1522 & 0.2155 & 0.2346 & 0.2444 & 0.2625 & 0.2607\\\hline
					Durchschnitt IoU $\neq 0$ & 0.4131 & 0.4094 & 0.4458 & 0.4222 & 0.4534& 0.4503\\
					
						
				\end{tabular}
				\caption{Die Tabelle zeigt die Bewertung von Models in Intersection-over-union (IoU) Metrik in Abhängigkeit der Trainingsdauer.}
				\label{lbl: IoUs}
			\end{table}
		
			\begin{figure}[h]
				\centering
				\includegraphics[width=15cm]{img/IoUMobilenet.png}
				\caption{Durchschnittlicher IoU in Abhängigkeit der Anzahl trainierter Epochen. Der Graph erreicht sein Maximum bei 25 Epochen und fällt danach leicht ab, was auf Overfitting schließen lässt.}
				\label{fig:IoUMn}
			\end{figure} 
		
			Wie in Tabelle \ref{lbl: IoUs} und Abbildung \ref{fig:IoUMn} zu erkennen ist, verbessert sich die Bewertung des Models bis zu einem Punkt, der in etwa bei Epoche 25 liegt und verschlechtert sich bei Verlängerung der Trainingsdauer wieder. Es tritt also trotz der L2 Regularisierung der Netzparameter und der Nutzung des Dropout-Verfahrens wahrscheinlich Overfitting auf. Im Folgenden wird das für 25 Epochen trainierte Netz verwendet. Die Bewertung dieses Models ist in Abbildung \ref{fig:KIoUMn} dargestellt.\\
			
			\begin{figure}[h!]
				\centering
				\includegraphics[width=15cm]{img/KlasseIoUMobilenet.png}
				\caption{IoU für jede Klasse vom Test des für 25 Epochen trainierten Netzes. Das Diagramm zeigt gute Ergebnisse für amorphe Klassen und schlechtere für kleine Objekte. Zu sehen ist auch, dass einige Klassen, die in den Trainingsdaten selten vorkommen oder Ähnlichkeit mit anderen Klassen aufweisen, nicht erkannt werden.}
				\label{fig:KIoUMn}
			\end{figure} 
		
			Die Models weisen durchwegs ihre höchsten Ergebnisse beim Erkennen der amorphen Klassen Straße, Gebäude, Vegetatio und Himmel auf, wie bei einem semantischen Segmentierungsverfahren zu erwarten. Das niedrige Ergebnisse bei der Klasse Gelände lässt sich dadurch erklären, dass der Cityscapes-Datensatz in städtischen Umgebungen aufgenommen wird, wo diese Klasse selten auftritt. Vergleichsweise hoch ist auch der IoU-Wert für die Klasse Auto, die in dem Datensatz besonders häufig ist.\\
			Die niedrigsten positiven IoU-Werte weisen die Ergebnisse bei kleineren, zählbaren Objekten wie Personen, Pfähle, Fahrräder und Verkehrszeichen auf. \\
			Die in Abbildung \ref{fig:KIoUMn} dargestellten Ergebnisse lassen außerdem erkennen, dass das Netz bestimmte Klassen praktisch nicht erkennt. Dies lässt vermuten, dass es nur eingeschränkt fähig ist, Bildsegmente anhand ihres Kontextes zu bewerten und beispielsweise zwischen einem Fußgänger und einem Fahrradfahrer oder zwischen einer Mauer und einem Gebäude zu unterscheiden und die häufiger auftretende Variante auswählt. Die Ergebnisse der Klasse Fahrrad lassen vermute, dass ein längeres Training dieses Verhalten verbessern könnte. Da ein zu langes Training sich, wie vorher erwähnt, negativ auf den durchschnittlichen IoU auswirkt, wird in diesem Experiment aber davon abgesehen. Eine weitere Möglichkeit wäre, dem Trainingssatz mehr Daten hinzuzufügen, die vermehrt die entsprechenden Objekte enthalten.\\
			
			\begin{figure}[h!]
				\centering
				\includegraphics[width=15cm]{img/BeispielMobilenet.png}
				\caption{Zu sehen sind Beispiele für Ausgaben des Models mit der besten Bewertung im vorherigen Experiment (25 Epochen trainiert).\\ Von links nach rechts gezeigt sind Eingabebild, Ground Truth, Ausgabe von DeepLab.}
				\label{fig:BspMn}
			\end{figure} 
			
			Abbildung \ref{fig:BspMn} zeigt ausgewählte Ausgaben des für 25 Epochen lang trainierten Netzes, das die besten Ergebnisse in der IoU-Metrik liefert. Sie spiegeln die Bewertung aus Tabelle \ref{lbl: IoUs} wieder, mit hoher Genauigkeit bei großen und amorphen Objekten und niedriger bei kleineren, zählbaren. Besonders auffällig sind False Positives der Klasse Person, die das Model oft an Fahrräder oder Bäume in der nähe von Personen vergibt.\\
			Die Beispiele lassen erkennen, dass das Netz teilweise die Trainingsdaten memorisiert. So wird dem oberen Teil eines Fahrrades häufig die Klasse Person zugewiesen und umgekehrt der untere Teil einer Person als Fahrrad erkannt. Genauso werden Bereiche, die sich über einem als Straße erkannten Segments befinden tendenziell eher als Auto klassifiziert. Das lässt sich auf die Funktion des CRF zurückführen.
		
	\subsection{Xeption65}
		Wir betrachten den Xception65-Backbone im Vergleich zu MobileNetV2. Auf ein Experiment mit unterschiedlichen Trainingsepochen wird an dieser Stelle aufgrund der langen Trainingsdauer der Netze verzichtet. Das verwendete Model ist 60 Epochen lang trainiert mit denselben Hyperparametern wie die Models mit MobileNetV2. Da in diesem Experiment die für ein Bild nötige Verarbeitungszeit eine Rolle spielt, soll hier erwähnt werden, dass der Rechner auf dem es durchgeführt wird mit der Grafikkarte NVIDIA GeForce GTX 1070, auf der die Berechnungen durchgeführt werden, ausgestattet ist und über 16GB RAM verfügt. 
		
		\begin{table}[h]
			\centering
			\begin{tabular}{l|cc|c}
				 & Xception65 & MobileNetV2 &  Differenz \\\hline
				IoU Straße & 0.6951 & 0.6774 & 0.0177 \\\hline
				IoU Gehsteig & 0.7238 & 0.5193 & 0.2045 \\\hline
				IoU Gebäude & 0.8533 & 0.6931 & 0.1602 \\\hline
				IoU Mauer & 0.1461 & 0.0 & 0.1461 \\\hline
				IoU Zaun & 0.1641 & 0.0 & 0.1641 \\\hline
				IoU Pfahl & 0.5843 & 0.2835 & 0.3008 \\\hline
				IoU Ampel & 0.3049 & 0.0 & 0.3049 \\\hline
				IoU Verkehrszeichen & 0.6592 & 0.2233 & 0.4359 \\\hline
				IoU Vegetation & 0.8558 & 0.7728 & 0.0830 \\\hline
				IoU Gelände & 0.2068 & 0.1593 & 0.0475 \\\hline
				IoU Himmel & 0.7707 & 0.7153 & 0.0554 \\\hline
				IoU Person & 0.5234 & 0.1680 & 0.3554 \\\hline
				IoU Radfahrer & 0.2386 & 0.0 & 0.2386 \\\hline
				IoU Auto & 0.8369 & 0.6419 & 0.1950 \\\hline
				IoU Lastwagen & 0.0691 & 0.0 & 0.0691 \\\hline
				IoU Bus & 0.0893 & 0.0 & 0.0893 \\\hline
				IoU Zug & 0.0185 & 0.0 & 0.0185 \\\hline
				IoU Motorrad & 0.0685 & 0.0 & 0.0685 \\\hline
				IoU Fahrrad & 0.4080 & 0.1340 & 0.4080 \\\hline
				Durchschnitt IoU & 0.4324 & 0.2625 & 0.2346 \\\hline
				Durchschnittliche & 764 & 387 & 377 \\
				Verarbeitungszeit [ms] & & & \\
				
			\end{tabular}
			\caption{Gezeigt ist ein Vergleich zwischen den Ergebnissen von DeepLab mit Xception65 und MobileNetV2 in IoU-Metrik und der Verarbeitungszeit.}
			\label{lbl: XcVsMn}
		\end{table}
	
		\begin{figure}[h!]
			\centering
			\includegraphics[width=15cm]{img/XceptionVsMobileNet.png}
			\caption{Der Vergleich von Xception65 und MobileNetV2 in IoU-Metrik zeigt, dass Xception die besten Ergebnisse bei denselben Klassen wie MobileNetV2 produziert, aber deutlich besser beim Erkennen von Objekten und Details ist.}
			\label{fig:XcVsMn}
		\end{figure} 
		
		Wie in Tabelle \ref{lbl: XcVsMn} und Abbildung \ref{fig:XcVsMn} zu sehen ist, erzeugt das Model mit Xception65 durchwegs bessere Ergebnisse als das mit MobileNetV2, benötigt aber im Durchschnitt 97\% mehr Zeit für die Verarbeitung eines Eingabebildes.\\
		Durch die Verwendung von Xception65 ist das Model außerdem in der Lage, alle Klassen des Datensatzes zu erkennen, auch wenn die IoU-Werte für die von MobileNetV2 nicht erkannten Klassen vergleichsweise niedrig sind. Eine besonders große Steigerung der Bewertung im Vergleich mit MobileNetV2 weist das Netz bei kleineren, zählbaren Objekten auf wie Personen (311\%) und Verkehrszeichen (295\%) auf.\\
		Abbildung \ref{fig:BspXc} zeigt beispielhafte Ergebnisse des Models aus dem Validierungs-Datensatz von Cityscapes.
		
		\begin{figure}[h!]
			\centering
			\includegraphics[width=15cm]{img/BeispielXception.png}
			\caption{Gezeigt sind Beispiele für Ausgaben des Models, das Xception65 nutzt.\\ Von links nach rechts zu sehen ist Eingabebild, Ground Truth, Ausgabe von DeepLab.}
			\label{fig:BspXc}
		\end{figure} 
		
		
		
	\section{Verfeinerung mit KITTI}
		In diesem Experiment wird das am besten bewertete, mit Cityscapes trainierte Model mit MobileNetV2 als Backbone mehrere Epochen mit den 200 Bildern umfassenden Trainingsdaten für semantische Segmentierung von KITTI nachtrainiert. Die Resultierenden Models werden anschließend mit anderen Bildern aus dem KITTI-Datensatz getestet. Da KITTI nur für diese 200 Bilder eine Ground Truth zur Verfügung stellt und ein Test auf den Trainingsdaten wenig aussagekräftig ist, begnügt sich das Experiment in diesem Fall mit einer empirischen Auswertung.\\
		Abbildung \ref{fig:Verf} zeigt beispielhaft je ein Bild aus beiden Datensätzen und die Ausgaben von DeepLab für verschieden lang nachtrainierte Models.  
		\begin{figure}[h!]
			\centering
			\includegraphics[width=12cm]{img/Verfeinerung.png}
			\caption{Beispiel für Ausgabe von mit KITTI-Daten verfeinerter Models. Die erste Epoche der Verfeinerung zeigt die größte Verbesserung auf den KITTI-Daten und gleichzeitig eine deutliche Verschlechterung bei Cityscapes-Bildern. Dieses Verhalten ist durch ein Nachtraining mit Cityscapes reversibel. Das Experiment zeigt, dass Overfitting ein Problem des Netzes ist.}
			\label{fig:Verf}
		\end{figure} 
		Wie man sieht, tritt bei den Ergebnissen auf dem KITTI-Datensatz bereits nach einer Epoche Verfeinerung mit KITTI-Daten eine deutliche Verbesserung auf. Weitere Trainingsepochen verbessern die Ergebnisse weiter, aber weniger erheblich. Gleichzeitig verschlechtert sich die Ausgabe für ein Bild aus dem Cityscapes-Datensatz in ähnlichem Maße. \\Das unterste Bild von Abbildung \ref{fig:Verf} zeigt die Ausgabe eines Models, das für 25 Epochen auf Cityscapes-Daten trainiert, für 5 Epochen mit KITTI-Daten verfeinert und anschließend für eine Epoche mit Cityscapes-Daten nachtrainiert ist. Wie man sieht, führt die Verfeinerung mit dem ursprünglichen Datensatz wieder zu einer Verschlechterung der Ergebnisse auf KITTI-Bildern und einer Verbesserung auf Cityscapes-Bildern.\\
		Diese Beobachtungen bestärken die Vermutung, dass beim Trainieren des Netzes Overfitting auftritt.\\
		
		In einem nächsten Schritt soll untersucht werden, wie die Berücksichtigung der KITTI-Daten beim gesamten Lernprozess die Ergebnisse auf beiden Datensätzen beeinflusst. Dazu wird der annotierte KITTI-Datensatz geteilt in einen Trainings- und Evaluierungssatz zu je 100 Bildern. Dadurch wird es möglich, eine Aussagekräftige Bewertung in IoU-Metrik zu berechnen. Aufbauend auf dem vorherigen Experiment wird ein Model mit gemischten Daten für 24 Epochen trainiert (B) und ausgewertet. Danach wird es für eine Epoche ausschließlich mit KITTI-Daten verfeinert (C) und dann für eine weitere Epoche mit dem gesamten, gemischten Trainingssatz nachtrainiert (D). Damit ist das letzte Model 25 Epochen auf den gemischten Daten trainiert und hat damit genauso viele Lernschritte auf Cityscapes-Bilder absolviert wie das am besten Bewertete Model von Abschnitt \ref{sec:mn2} (A). Die Ergebnisse sind in Tabelle 6.3 eingetragen und in Abbildung \ref{fig:Verg} graphisch dargestellt. 
		 
		\begin{table}
			\centering
			\begin{tabular}{p{4cm}|m{3cm}m{3cm}}
				
				& Durchschnittlicher IoU & Durchschnittlicher IoU \\
				Trainingsverhalten&  auf Cityscapes-Bildern & auf KITTI-Bildern\\\hline
				25 Epochen mit Cityscapes-Daten (A)& 0.2625 & 0.1681\\\hline
				24 Epochen mit gemischten Daten (B)& 0.2605 & 0.2002\\\hline
				24 Epochen mit gemischten Daten + 1 Epoche mit KITTI-Daten (C)& 0.2045 & 0.2350\\\hline
				24 Epochen mit gemischten Daten + 1 Epoche mit KITTI-Daten + 1Epoche mit gemischten Daten (D)& 0.2718 & 0.2105
				
				
			\end{tabular}
			\label{res:mix}
			\caption{Die Tabelle zeigt die Bewertung von Netzwerken mit unterschiedlich verfeinerten Trainingsdaten und -verhalten auf Evaluierungssätzen von Cityscapes und KITTI.}
		\end{table}
		
		\begin{figure}[h!]
			\centering
			\includegraphics[width=12cm]{img/VergleichCK.png}
			\caption{Graphische Darstellung der Ergebnisse von Tabelle 6.3. Besonders auffällig sind die Unterschiede bei den Ergebnissen auf den KITTI-Daten (rot) und der Einbruch bei denen der Cityscapes-Bilder (blau) in Model C.}
			\label{fig:Verg}
		\end{figure} 
		
		
		Verglichen mit Model A zeigen die Ergebnisse bei Model B deutlich bessere Ergebnisse bei den KITTI-Daten, während die auf den Cityscapes-Daten etwa gleich sind. In Model C zeigt sich ein ähnliches Verhalten wie im vorherigen Experiment mit einer deutlichen Verbesserung bei Den KITTI-Daten und einer Verschlechterung bei den Cityscapes-Daten. Die Ergebnisse von Model D weisen auf beiden Datensätzen eine Verbesserung gegenüber denen von Model A auf, wobei der unterschied bei den Bildern von KITTI deutlich größer ist. Allerdings verschlechtern sich die Ergebnisse von Model D bei den KITTI-Daten bezüglich Model C.\\
		Die Ergebnisse zeigen, dass das Hinzufügen zusätzlicher Trainingsdaten, was zu einer größeren Varietät darin führt, einen positiven Effekt auf die Leistung des Netzes hat.\\
		Abbildung \ref{fig:VergMix} zeigt die Ausgabe der verschiedenen Models auf den Bildern vom vorherigen Versuch, einem aus dem KITTI-, einem aus dem Cityscapes-Datensatz. Bei den Ergebnissen des Cityscapes-Bildes zeigen die Models A, B und D ähnliche Resultate und Model C ein deutlich schlechteres, was die Bewertung aus Tabelle \ref{res:mix} widerspiegelt. Das Resultat von Model C ist jedoch augenscheinlich besser als die der mit KITTI verfeinerten Models in Abbildung \ref{fig:Verf}. Bei dem Bild aus KITTI liefern Models B, C und D erwartungsgemäß deutlich bessere Ergebnisse als Model A. Das empirisch beste Resultat erzielt Model C, das aber das schlechteste auf dem Cityscapes-Bild produziert. Die Ergebnisse von Model D zeigen eine geringfügige, aber erkennbare Verbesserung gegenüber Model B.\\
		Dieses Experiment zeigt, dass eine hohe Varietät innerhalb der Trainingsdaten wünschenswert ist. Ist die Art der Daten bekannt, auf denen das Netz angewandt werden soll, ist es zudem ratsam, es mit annotierten Beispieldaten zu verfeinern.
	
		
		\begin{figure}[h!]
			\centering
			\includegraphics[width=12cm]{img/VergleichMix.png}
			\caption{Hier sieht man beispielhaft Ergebnisse für ein Bild aus dem KITTI- und eines aus dem Cityscapes-Datensatz. Die Bilder sind dieselben wie in Abbildung \ref{fig:Verf}, um einen Vergleich zu ermöglichen. Wie man sieht, zeigen die Models A, B und D ähnlich gute Ergebnisse auf dem Cityscapes-Bild. Auf dem KITTI-Bild erzielt die besten Ergebnisse das Model C, das die schlechtesten auf Cityscapes liefert. Auf dem KITTI-Bild erzeugen auch die Models B und D bessere Resultate als Model A, das komplett ohne KITTI-Daten trainiert ist.}
			\label{fig:VergMix}
		\end{figure} 
	
	\section{Segmentierung von Punktwolken}
		In diesem Abschnitt sollen die Ergebnisse der Punktwolkensegmentierung diskutiert werden. Wie bereits erwähnt, werden die Experimente auf LiDAR-Daten des KITTI-Datensatzes durchgeführt. Da diese nicht annotiert sind, werden die Ergebnisse an dieser Stelle nur empirisch ausgewertet. In diesem Experiment wird das Model C aus dem letzten Abschnitt verwendet, das 24 Epochen auf einer Mischung aus Cityscapes- und KITTI-Daten und eine Epoche nur auf KITTI-Daten trainiert ist und die höchste IoU-Bewertung auf den KITTI-Bildern erzielt. In Abbildung \ref{fig:BspPWX1} und \ref{fig:BspPWX2} sind zwei Beispiele für segmentierte Punktwolken analog zu Abbildung \ref{fig:BspPW} gezeigt.\\
		\begin{figure}[h!]
			\centering
			\includegraphics[width=11cm]{img/BspPWSeg1.png}
			\caption{Beispiel für segmentierte Punktwolken. Zu sehen sind von oben nach unten: Originalbild, segmentiertes Bild, segmentierte Punktwolke aus vier verschiedenen Perspektiven. Gezeigt ist eine Szene, in der mehrere Fahrzeuge auf einer dreispurigen Straße an eine rote Ampel heranfahren. Im Hintergrund ist Vegetation. Die Segmentierung der Straße und Autos in der Punktwolke ist weitgehend Korrekt. Die Lastwägen werden im unteren Teil als Autos, im oberen als Gebäude oder Vegetation klassifiziert.}
			\label{fig:BspPWX1}
		\end{figure}
		\begin{figure}[h!]
			\centering
			\includegraphics[width=11cm]{img/BspPWSeg2.png}
			\caption{Beispiel für segmentierte Punktwolken. Zu sehen sind von oben nach unten: Originalbild, segmentiertes Bild, segmentierte Punktwolke aus vier verschiedenen Perspektiven. Gezeigt ist eine Szene an einer Kreuzung mit vielen Pfahl-artigen Objekten. Das filmende Fahrzeug hält an einer roten Ampel, während ein Auto die Kreuzung von links nach rechts durchfährt. Ein weiteres Auto fährt von links in die Bildfläche ein. Die Segmentierung des Autos auf dem Bild weist Fehler im oberen Bereich auf, die auch in der segmentierten Punktwolke wiederzufinden sind. Das nicht gut sichtbare Fahrzeug am linken Rand wird vom Netzwerk nicht korrekt erkannt. Die Pfähle werden in der Punktwolke gut segmentiert, obwohl sie im Bild nur ungenau erkannt werden.}
			\label{fig:BspPWX2}
		\end{figure} 
		Offensichtlich ist die Genauigkeit der Punktwolkensegmentierung abhängig von der Qualität der Bildsegmentierung, was besonders in Abbildung \ref{fig:BspPWX1} deutlich wird. Aufgrund der in Abschnitt \ref{sec:mn2} bestimmten Eigenschaft von MobileNetV2, nur häufig vorkommende Klassen des Trainingssets zu erkennen, werden die Lastwägen im Bild im unteren Bereich als Autos und im oberen Bereich als Mischung aus Gebäuden und Vegetation erkannt, was dann in die Punktwolke projiziert wird.\\
		Insgesamt zeigen die Beispiele gute Ergebnisse bei zählbaren Objekten im Vordergrund, selbst wenn die Bildsegmentierung in diesen Bereichen nicht sehr präzise ist, wie in Abbildung \ref{fig:BspPWX2}. Die Segmentierung der Pfähle darin ist verschwommen und weist keine klaren Ränder auf. In der Punktwolke führt dies dazu, dass die Objekte selbst richtig und der Hintergrund hinter den Objekten falsch gelabelt wird.\\
		Hinsichtlich dieser Eigenschaften wäre ein Netz wünschenswert, das die Klasse zählbarer Objekte großzügig verteilt, wodurch diese in den Punktwolken richtig segmentiert werden. Die durch verschwommene Ränder im segmentierten Bild falsch erkannten Punkte in amorphen Objekten fallen im Vergleich zu der Menge an Punkten dieser Klasse nicht ins Gewicht. Dazu kommt, dass diese sich oft im Hintergrund befinden und für praktische Anwendungen von geringerem Interesse sind.
	
	\section{Aufgetretene Probleme und Lösungen}
		\subsection{False Positives}
			Models, die MobileNetV2 benutzen neigen dazu, einige Klassen zum Großteil mit einer anderen zu klassifizieren. Beispiele dafür sind die bereits angesprochene Erkennung von Fahrrädern als Personen und die Klassifizierung großer Fahrzeuge als Gebäude. \\
			Bei Verwendung des Xception65-Backbones kommt es reproduzierbar bei bestimmten Bildern zu einem Phänomen, bei dem eine große Anzahl Pixel nahe einer der Ecken als Straße beziehungsweise die Klasse mit dem niederwertigsten Label klassifiziert wird. Es besteht kein erkennbarer Zusammenhang zwischen den Bildern, bei denen dieses Verhalten auftritt. Abbildung \ref{fig:BspBadXc} zeigt Beispiele dafür.
			\begin{figure}[h!]
				\centering
				\includegraphics[width=15cm]{img/BeispielBadXception.png}
				\caption{Hier zu sehen sind Beispiele für Ausgaben des Models mit Xception65, die das oben beschriebene Phänomen aufweisen.\\ Von links nach rechts gezeigt sind Eingabebild, Ground Truth, Ausgabe von DeepLab. Das Verhalten ist auf den jeweiligen Bildern reproduzierbar.}
				\label{fig:BspBadXc}
			\end{figure} 
		\subsection{Overfitting}
			Overfitting hat sich als eines der hauptsächlichen Probleme bei der Verwendung von MobileNetV2 herausgestellt. Schon kleine Änderungen in der Perspektive, wie sie beispielsweise im KITTI-Datensatz auftritt, verursachen eine spürbare Verschlechterung der Ergebnisse. \\
			Um bessere Ergebnisse auf dem KITTI-Datensatz zu erzeugen, hat es sich als vorteilhaft herausgestellt, den Trainingssatz für semantische Segmentierung der KITTI-Daten beim Trainieren des Netzes mitzuberücksichtigen. Das Netz mit jenen Daten nachzutrainieren verbessert ebenfalls die Ergebnisse, kann aber wiederum zu Overfitting auf diesen Datensatz führen. Es kann hilfreich sein, nach dem Nachtraining noch eine Epoche mit allen verfügbaren Daten nachzutrainieren. Allgemein wirkt sich eine hohe Varietät innerhalb der Trainingsdaten positiv aus.\\
			Weitere Möglichkeiten, gegen Overfitting vorzugehen, mit denen in dieser Arbeit aber nicht experimentiert wird, sind eine Erhöhung der Dropout-Rate und dem L2 Regularisierungsfaktor.