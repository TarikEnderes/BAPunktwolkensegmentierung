\chapter{Experimente}
	\section{Technische Daten des für die Experimente verwendeten Rechners}
		\begin{itemize}
			\item Prozessor: Intel(R) Core(TM) i7-7700HQ CPU @ 2.860GHz
			\item Grafikkarte: NVIDIA GeForce GTX 1070
			\item Arbeitsspeicher: 16GB RAM
		\end{itemize}
	\section{Backbones}
		Für die Durchführung folgender Experimente wird der fein annotierte Cityscapes-Datensatz verwendet. Die Netze werden auf dem aus 2975 Bildern bestehenden Trainingssatz trainiert und auf dem 500 Bilder fassenden Validierungssatz ausgewertet.\\
		Für jede Trainingsepoche werden aus dem Trainingssatz 1487 Bilder zufällig ausgewählt. Als Grundlernrate wird 0.007 gewählt. Der Trainingsfehler wird über die in \cite{Lovasz} beschriebene Lovasz-Funktion berechnet. Weitere Hyperparameter sind eine Dropout-Rate von 0.1,  eine L2 Regularisierungsrate von $4*10^{-5}$ und ein Momentum-Faktor von 0.9.
		
		\subsection{MobileNetV2}
			Da Echtzeitfähigkeit eine wichtige Rolle spielt, konzentrieren sich die Experimente auf das leichtgewichtige MobileNetV2, statt dem leistungsfähigeren Xception65.
			%\subsubsection{Auswirkungen der Trainingsdauer}
			\begin{table}[h]
			
				\begin{tabular}{l|cccccc}
					Epochen trainiert & 1 & 5 & 10 & 15 & 25 & 30\\\hline
					IoU Straße & 0.6260 & 0.6498 & 0.6727 & 0.6729 & 0.6774 & 0.6675\\\hline
					IoU Gehsteig & 0.1835 & 0.3788 & 0.4330 & 0.4616 & 0.5193 & 0.5078\\\hline
					IoU Gebäude & 0.5563 & 0.6402 & 0.6573 & 0.6771 & 0.6931 & 0.7068\\\hline
					IoU Mauer & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Zaun & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Pfahl & 0.0791 & 0.1931 & 0.2159 & 0.2561 & 0.2835 & 0.2728\\\hline
					IoU Ampel & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Verkehrszeichen & 0.0 & 0.1056 & 0.1785 & 0.1859 & 0.2233 & 0.2291\\\hline
					IoU Vegetation & 0.5581 & 0.7153 & 0.7273 & 0.7518 & 0.7728 & 0.7663\\\hline
					IoU Gelände & 0.0 & 0.0915 & 0.1274 & 0.1355 & 0.1593 & 0.1445\\\hline
					IoU Himmel & 0.5353 & 0.6606 & 0.6867 & 0.6977 & 0.7153 & 0.7081\\\hline
					IoU Person & 0.0 & 0.1160 & 0.1582 & 0.1458 & 0.1680 & 0.1863\\\hline
					IoU Radfahrer & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Auto & 0.3537 & 0.5438 & 0.6014 & 0.5863& 0.6419 & 0.6103\\\hline
					IoU Lastwagen & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Bus & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Zug & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Motorrad & 0.0 & 0.0 & 0.0 & 0.0 & 0.0 & 0.0\\\hline
					IoU Fahrrad & 0.0 & 0.0 & 0.0 & 0.0750 & 0.1340 & 0.1542\\\hline
					Durchschnitt IoU & 0.1522 & 0.2155 & 0.2346 & 0.2444 & 0.2625 & 0.2607\\\hline
					Durchschnitt IoU $\neq 0$ & 0.4131 & 0.4094 & 0.4458 & 0.4222 & 0.4534& 0.4503\\
					
						
				\end{tabular}
				\caption{Bewertung von Models in Intersection-over-union (IoU) Metrik in Abhängigkeit der Trainingsdauer. Berechnung des IoU durch: IoU = True Positives / (True Positives + False Positives + False Negatives)}
				\label{lbl: IoUs}
			\end{table}
		
			\begin{figure}[h]
				\centering
				\includegraphics[width=15cm]{img/IoUMobilenet.png}
				\caption{Durchschnittlicher IoU in Abhängigkeit der Anzahl trainierter Epochen}
				\label{fig:IoUMn}
			\end{figure} 
		
			Wie in Tabelle \ref{lbl: IoUs} und Abbildung \ref{fig:IoUMn} zu erkennen ist, verbessert sich die Bewertung des Models bis zu einem Punkt, der in etwa bei Epoche 25 liegt und verschlechtert sich bei Verlängerung der Trainingsdauer wieder. Es tritt also trotz der L2 Regularisierung der Netzparameter und der Nutzung des Dropout-Verfahrens wahrscheinlich Overfitting auf. Im Folgenden wird das für 25 Epochen trainierte Netz verwendet. Die Bewertung dieses ist in Abbildung \ref{fig:KIoUMn} dargestellt.\\
			
			\begin{figure}
				\centering
				\includegraphics[width=15cm]{img/KlasseIoUMobilenet.png}
				\caption{IoU für jede Klasse vom Test des für 25 Epochen trainierten Netzes}
				\label{fig:KIoUMn}
			\end{figure} 
		
			Die Models weisen durchwegs ihre höchsten Ergebnisse beim Erkennen der amorphen Klassen Straße, Gebäude, Vegetatio und Himmel auf, wie bei einem semantischen Segmentierungsverfahren zu erwarten. Das niedrige Ergebnisse bei der Klasse Gelände lässt sich dadurch erklären, dass der Cityscapes-Datensatz in städtischen Umgebungen aufgenommen wird, wo diese Klasse selten auftritt. Vergleichsweise hoch ist auch der IoU-Wert in der Klasse Auto, die in dem Datensatz besonders häufig ist.\\
			Die niedrigsten positiven IoU-Werte weisen die Ergebnisse bei kleineren, zählbaren Objekten wie Personen, Pfähle, Fahrräder und Verkehrszeichen auf. \\
			Die in Abbildung \ref{fig:KIoUMn} dargestellten Ergebnisse lassen außerdem erkennen, dass das Netz bestimmte Klassen praktisch nicht erkennt. Dies lässt vermuten, dass es nur eingeschränkt fähig ist, Bildsegmente anhand ihres Kontextes zu bewerten und beispielsweise zwischen einem Fußgänger und einem Fahrradfahrer oder zwischen einer Mauer und einem Gebäude zu unterscheiden und die häufiger auftretende Variante auswählt. Die Ergebnisse der Klasse Fahrrad lassen vermute, dass ein längeres Training dieses Verhalten verbessern könnte. Da ein zu langes Training sich, wie vorher erwähnt, negativ auf den durchschnittlichen IoU auswirkt, wird in diesem Experiment aber davon abgesehen. Eine weitere Möglichkeit wäre, dem Trainingssatz mehr Daten hinzuzufügen, die vermehrt die entsprechenden Objekte enthalten.\\
			
			\begin{figure}
				\centering
				\includegraphics[width=15cm]{img/BeispielMobilenet.png}
				\caption{Beispiele für Ausgaben des Models (25 Epochen trainiert)\\ Von links nach rechts: Eingabebild, Ground Truth, Ausgabe von DeepLab}
				\label{fig:BspMn}
			\end{figure} 
			
			\ref{fig:BspMn} zeigt ausgewählte Ausgaben des für 25 Epochen lang trainierten Netzes, das die besten Ergebnisse in der IoU-Metrik liefert. Sie spiegeln die Bewertung in \ref{lbl: IoUs} mit hoher Genauigkeit bei großen und amorphen Objekten und niedriger bei kleineren, zählbaren. Besonders auffällig sind False Positives der Klasse Person, die das Model oft an Fahrräder oder Bäume in der nähe von Personen vergibt.\\
			Die Beispiele lassen erkennen, dass das Netz teilweise die Trainingsdaten memorisiert. So wird dem oberen Teil eines Fahrrades häufig die Klasse Person zugewiesen und umgekehrt der untere Teil einer Person als Fahrrad erkannt. Genauso werden Bereiche, die sich über einem als Straße erkannten Segments befinden tendenziell eher als Auto klassifiziert.
		
	\subsection{Xeption65}
		Wir betrachten den Xception65-Backbone im Vergleich zu MobileNetV2. Auf ein Experiment mit unterschiedlichen Trainingsepochen wird an dieser Stelle aufgrund der langen Trainingsdauer der Netze verzichtet. Das verwendete Model ist 60 Epochen lang trainiert mit denselben Hyperparametern wie die Models mit MobileNetV2.
		
		\begin{table}[h]
			
			\begin{tabular}{l|cc|c}
				 & Xception65 & MobileNetV2 &  Differenz \\\hline
				IoU Straße & 0.6951 & 0.6774 & 0.0177 \\\hline
				IoU Gehsteig & 0.7238 & 0.5193 & 0.2045 \\\hline
				IoU Gebäude & 0.8533 & 0.6931 & 0.1602 \\\hline
				IoU Mauer & 0.1461 & 0.0 & 0.1461 \\\hline
				IoU Zaun & 0.1641 & 0.0 & 0.1641 \\\hline
				IoU Pfahl & 0.5843 & 0.2835 & 0.3008 \\\hline
				IoU Ampel & 0.3049 & 0.0 & 0.3049 \\\hline
				IoU Verkehrszeichen & 0.6592 & 0.2233 & 0.4359 \\\hline
				IoU Vegetation & 0.8558 & 0.7728 & 0.0830 \\\hline
				IoU Gelände & 0.2068 & 0.1593 & 0.0475 \\\hline
				IoU Himmel & 0.7707 & 0.7153 & 0.0554 \\\hline
				IoU Person & 0.5234 & 0.1680 & 0.3554 \\\hline
				IoU Radfahrer & 0.2386 & 0.0 & 0.2386 \\\hline
				IoU Auto & 0.8369 & 0.6419 & 0.1950 \\\hline
				IoU Lastwagen & 0.0691 & 0.0 & 0.0691 \\\hline
				IoU Bus & 0.0893 & 0.0 & 0.0893 \\\hline
				IoU Zug & 0.0185 & 0.0 & 0.0185 \\\hline
				IoU Motorrad & 0.0685 & 0.0 & 0.0685 \\\hline
				IoU Fahrrad & 0.4080 & 0.1340 & 0.4080 \\\hline
				Durchschnitt IoU & 0.4324 & 0.2625 & 0.2346 \\\hline
				Durchschnittliche & 764 & 387 & 377 \\
				Verarbeitungszeit [ms] & & & \\
				
			\end{tabular}
			\caption{Vergleich zwischen Xception65 und MobileNetV2 in IoU-Metrik und Verarbeitungszeit}
			\label{lbl: XcVsMn}
		\end{table}
	
		\begin{figure}
			\centering
			\includegraphics[width=15cm]{img/XceptionVsMobileNet.png}
			\caption{Vergleich Xception65 und MobileNetV2 in IoU-Metrik}
			\label{fig:XcVsMn}
		\end{figure} 
		
		Wie in Tabelle \ref{lbl: XcVsMn} und Abbildung \ref{fig:XcVsMn} zu sehen ist, erzeugt das Model mit Xception65 durchwegs bessere Ergebnisse als das mit MobileNetV2, benötigt aber im Durchschnitt 97\% mehr Zeit für die Verarbeitung eines Eingabebildes.\\
		Durch die Verwendung von Xception65 ist das Model außerdem in der Lage, alle Klassen des Datensatzes zu erkennen, auch wenn die IoU-Werte für die von MobileNetV2 nicht erkannten Klassen vergleichsweise niedrig sind. Eine besonders große Steigerung der Bewertung im Vergleich mit MobileNetV2 weist das Netz bei kleineren, zählbaren Objekten auf wie Personen (311\%) und Verkehrszeichen (295\%).\\
		Abbildung \ref{fig:BspXc} zeigt beispielhafte Ergebnisse des Models aus dem Validierungs-Datensatz von Cityscapes.
		
		\begin{figure}[h]
			\centering
			\includegraphics[width=15cm]{img/BeispielXception.png}
			\caption{Beispiele für Ausgaben des Models\\ Von links nach rechts: Eingabebild, Ground Truth, Ausgabe von DeepLab}
			\label{fig:BspXc}
		\end{figure} 
		
		
		
	\section{Verfeinerung mit KITTI}
		In diesem Experiment wird das am besten bewertete, mit Cityscapes trainierte Model mit MobileNetV2 als Backbone mehrere Epochen mit den 200 Bildern umfassenden Trainingsdaten für semantische Segmentierung von KITTI nachtrainiert. Die Resultierenden Models werden anschließend mit anderen Bildern aus dem KITTI-Datensatz getestet. Da KITTI nur für diese 200 Bilder eine Ground Truth zur Verfügung stellt und ein Test auf den Trainingsdaten wenig aussagekräftig ist, ist es nicht möglich, die Ergebnisse mit der IoU-Metrik zu bewerten. Das Experiment begnügt sich deshalb mit einer empirischen Auswertung. \\
		Abbildung \ref{fig:Verf} zeigt beispielhaft je ein Bild aus beiden Datensätzen und die Ausgaben von DeepLab für verschieden lang nachtrainierte Models.  
		\begin{figure}[h]
			\centering
			\includegraphics[width=15cm]{img/Verfeinerung.png}
			\caption{Beispiel für Ausgabe der verfeinerter Models}
			\label{fig:Verf}
		\end{figure} 
		Wie man sieht, tritt bei den Ergebnissen auf dem KITTI-Datensatz bereits nach einer Epoche Verfeinerung mit KITTI-Daten eine deutliche Verbesserung auf. Weitere Trainingsepochen verbessern die Ergebnisse weiter, aber weniger erheblich. Gleichzeitig verschlechtert sich die Ausgabe für ein Bild aus dem Cityscapes-Datensatz in ähnlichem Maße. \\Das unterste Bild von Abbildung \ref{fig:Verf} zeigt die Ausgabe eines Models, das für 25 Epochen auf Cityscapes-Daten trainiert, für 5 Epochen mit KITTI-Daten verfeinert und anschließend für eine Epoche mit Cityscapes-Daten nachtrainiert ist. Wie man sieht, führt die Verfeinerung mit dem ursprünglichen Datensatz wieder zu einer Verschlechterung der Ergebnisse auf KITTI-Bildern und einer Verbesserung auf Cityscapes-Bildern.\\
	\section{Aufgetretene Probleme und Lösungen}
		\subsection{False Positives}
			Models, die MobileNetV2 benutzen neigen dazu, einige Klassen zum Großteil mit einer anderen zu klassifizieren. Beispiele dafür sind die bereits angesprochene Erkennung von Fahrrädern als Personen und die Klassifizierung großer Fahrzeuge als Gebäude. \\
			Bei Verwendung des Xception65-Backbones kommt es reproduzierbar bei bestimmten Bildern zu einem Phänomen, bei dem eine große Anzahl Pixel nahe einer der Ecken als Straße klassifiziert wird. Es besteht kein erkennbarer Zusammenhang zwischen den Bildern, bei denen dieses Verhalten auftritt. Abbildung \ref{fig:BspBadXc} zeigt Beispiele dafür.
			\begin{figure}
				\centering
				\includegraphics[width=15cm]{img/BeispielBadXception.png}
				\caption{Beispiele für fehlerhafte Ausgaben des Models mit Xception65\\ Von links nach rechts: Eingabebild, Ground Truth, Ausgabe von DeepLab}
				\label{fig:BspBadXc}
			\end{figure} 
		\subsection{Overfitting}
			Overfitting hat sich als eines der hauptsächlichen Probleme bei der Verwendung von MobileNetV2 herausgestellt. Schon kleine Änderungen in der Perspektive, wie sie beispielsweise im KITTI-Datensatz auftritt, verursachen eine spürbare Verschlechterung der Ergebnisse. \\
			Um bessere Ergebnisse auf dem KITTI-Datensatz zu erzeugen, hat es sich als vorteilhaft herausgestellt, den Trainingssatz für semantische Segmentierung der KITTI-Daten beim Trainieren des Netzes mitzuberücksichtigen. Das Netz mit jenen Daten nachzutrainieren verbessert ebenfalls die Ergebnisse, kann aber wiederum zu Overfitting auf diesen Datensatz führen. Es kann hilfreich sein, nach dem Nachtraining noch eine Epoche mit allen verfügbaren Daten nachzutrainieren.\\
			Weitere Möglichkeiten, gegen Overfitting vorzugehen, mit denen in dieser Arbeit aber nicht experimentiert wird, sind eine Erhöhung der Dropout-Rate und dem L2 Regularisierungsfaktor.