\chapter{Literatur}
	\section{Digitale Bildverarbeitung}
		Das Feld der digitalen Bildverarbeitung umfasst nach \cite{gonzalez2008digital} die Verbesserung der Bildinformation für menschliche Betrachter, sowie die Verarbeitung von Daten aus Bildern zur Übertragung oder autonomen maschinellen Erkennung durch digitale Rechner. \\
		Der erste bekannte Einsatz digitaler Bildverarbeitung fand 1964 im Zuge des Raumfahrprogramms der USA statt, wobei ein Computer eingesetzt wurde, um Störungen in Bilder von der Mondoberfläche zu Korrigieren. Eine weitere Nennenswerte Entwicklung auf dem Gebiet ist die Erstellung von Röntgenbildern im Jahr 1979. Die Bedeutung digitaler Bildverarbeitung in der Geschichte steht in einem proportionalen Verhältnis zu den Möglichkeiten der Datenübertragung und -speicherung. Analog dazu erreicht sie ihren Durchbruch mit dem Aufkommen des World Wide Web um 1989.\\
		Zu den Aufgabe digitaler Bildverarbeitung gehören Bildbearbeitung, Bildaufbereitung, Bildkompression, Bildsegmentierung, Repräsentation und Objekterkennung. Die letzten drei Kategorien können einem Unterbereich der Digitalen Bildverarbeitung, dem maschinellen Sehen \cite{Forsyth:2002:CVM:580035} zugeteilt werden.
		
		\subsection{Maschinelles Sehen}
			Maschinelles Sehen ist der Bereich der digitalen Bildverarbeitung, der sich mit der automatischen Analyse und Auswertung von Bilddaten mit Hilfe von Computern befasst. Zu den häufigsten Einsatzgebieten zählen die Automation, beispielsweise in Landwirtschaft \cite{MANH2001139} und Industrie \cite{Baird:1977:IST:1622943.1622976}, und Mensch-Maschine-Schnittstellen, wie zum Beispiel in wahrnehmungsgesteuerten Benutzerschnittstellen \cite{Bradski98computervision} und Augmented Reality \cite{FeaturePointExt}. Natürlich spielt maschinelles Sehen auch eine wichtige Rolle beim autonomen Fahren \cite{DICKMANNS1987221}.
			Das maschinelle Sehen kann unterteilt werden in:
			\begin{description}
			
				\item[Bildsegmentierung] Bei der Segmentierung wird versucht, Punkte oder Bereiche von besonderem Interesse in einem Bild zu finden, um das Bild maschinell erkennbar zu machen. Für mehr Informationen siehe Abschnitt \ref{sec:digseg}.
				\item[Representation und Beschreibung] Dieser Bereich der Bildverarbeitung beschäftigt sich mit der Anfertigung mathematischer Beschreibungen von Bildern wie zum Beispiel die statistische Verteilung gerichteter Linien. Ein häufiges Mittel zur Beschreibung von Bildern ist "`Scale Invariant Feature Transform"' (SIFT) \cite{Lowe:1999:ORL:850924.851523}. Dabei handelt es sich um einen Algorithmus, der durch eine Reihe an Filter-Operationen aus einem Bild eine Menge von lokalen Feature-Vektoren berechnet, die in gewissen Maße invariant gegenüber Translation, Rotation, Größe und Helligkeit sind.
				\item[Objekterkennung] Ziel der Objekterkennung ist es, Objekte auf Bildern zu detektieren, sodass dargestellte Szenen maschinell erkannt werden können. Übliche Methoden sind Bildvergleiche und Deep Learning. Ein Konkretes Beispiel für einen Objekterkennungs-Algorithmus ist "`You Only Look One"' (YOLO) \cite{Redmon_2016_CVPR}, das Bounding Boxes und Klassifizierungen für ein Bild mit Hilfe eines Convolutional Neural Networks berechnet.
			\end{description}
	
	\section{Digitale Bildsegmentierung} \label{sec:digseg}
		Die automatische Segmentierung digitaler Bilder zählt zu den kompliziertesten Problemen der digitalen Bildverarbeitung und ist, wie in \cite{DBLP:journals/corr/abs-1904-09172} erläutert, unerlässlich für das Ziel der Objekterkennung. Aufgrund der Komplexität der Aufgabe verwenden viele moderne Methoden Neuronale Netze zu diesem Zweck. Als besonders geeignet gelten auf Convolutional Neural Networks basierende Architekturen. Nach wie vor kommen auch klassische Methoden der Bildsegmentierung zum Einsatz, wenn die Anforderungen an das System nicht hoch sind.\\
		
		\subsection{Klassische Methoden der Segmentierung}
			Als klassisch werden im Zuge dieser Arbeit alle Methoden der Segmentierung bezeichnet, die keine Prinzipien der Neuroinformatik ausnutzen und stattdessen auf den Grundlagen von Diskontinuität und Ähnlichkeit basieren. Dazu gehören nach \cite{gonzalez2008digital}:
			\begin{description}
				\item[Detektion von Diskontinuitäten] Auch das Detektieren von Punkten, Linien und Kanten, die unter dem Begriff Diskontinuitäten zusammengefasst werden können, gehört zum Bereich der Bildsegmentierung. Um das zu erreichen werden im Bild nach Stellen gesucht, an denen sich der Farbwert der Pixel räumlich rapide ändert. Im einfachsten Fall kann dies mittels Faltung mit einem Filter bewerkstelligt werden, der auf starke Änderungen des Farbwertes in eine bestimmte Richtung reagiert.
				\item[Segmentierung nach Schwellwerten] Die trivialste Methode der Segmentierung ist die nach Schwellwerten. Dabei werden Pixel oder Superpixel anhand ihres Farbwertes eingeordnet.
				\item[Regionen-basierte Segmentierung] Bei Regionen-basierter Segmentierung werden räumliche Homogenitätskriterien ausgenutzt, um ein Bild in Bereiche einzuteilen. Die zwei verbreitetsten klassischen Ansätze dafür sind "`Region Growing"' und "`Region Splitting and Merging"'.\\
				Beim \textit{Region Growing} werden eine Anzahl von Pixel im Bild ausgewählt. Die an diese angrenzenden Pixel werden untersucht und der Region hinzugefügt, wenn sie die entsprechenden Bedingungen erfüllen. Dann werden die Nachbarpixel der hinzugefügten Pixel untersucht und auf diese Weise die Region rekursiv aufgebaut.\\
				Beim \textit{Region Splitting and Merging} wird das Bild in Bereiche aufgeteilt bis alle damit entstandenen Regionen intern die Homogenitätsbedingungen erfüllen (Splitting). Danach werden benachbarte Regionen auf Ähnlichkeit untersucht und gegebenenfalls zusammengeführt (Merging).
				\item[Wasserscheide-Verfahren] Das Wasserscheide(Watershed)-Verfahren nutzt lokale Minima und Maxima aus, um Regionen zu bilden. Das (Graustufen-)Bild wird dazu als dreidimensional betrachtet, wobei der Farbwert als Höhe interpretiert wird. Man stelle sich vor, die so entstandene Struktur werde ausgehend von den lokalen Minima geflutet und dort abgegrenzt, wo sich zwei Wasserflächen treffen würden. Auf diese Weise entstehen Regionen im Bild anhand der Lage von eindimensionalen lokalen Maxima, die zweidimensionale miteinander verbinden. 
				\item[Segmentierung anhand von Bewegung] Bei der Verarbeitung von Videodaten kann auch die zwischen den einzelnen Bildern stattfindende Bewegung zur Segmentierung ausgenutzt werden. Dazu gibt es zahlreiche Ansätze. Die einfachsten Fälle sind Verfahren, bei denen Bilder pixelweise miteinander Verglichen werden, um ein Differenzbild zu erstellen, wie es beispielsweise bei Background Subtraction der Fall ist. Dabei geht der Algorithmus davon aus, dass die aufnehmende Kamera statisch und der Hintergrund damit unbeweglich ist. Pixel deren Farbwerte sich rapide ändern werden dementsprechend als Vordergrundobjekte gewertet.
			\end{description}
			
		\subsection{Segmentierung mit Neuronalen Netzen}
			Wie bereits erwähnt sind Neuronale Netze ein beliebtes Mittel zur digitalen Bildsegmentierung. Auf einige Ansätze in diesem Gebiet soll hier eingegangen werden. Oft ist es sinnvoll, Methoden die Machine Learning verwenden danach einzuteilen, wie intensiv der Lernvorgang von einer Person überwacht werden muss. An dieser Stelle wird unterschieden zwischen unüberwachtem, schwach überwachtem und überwachtem Lernen.\\
			Unüberwachtes Lernen findet Anwendung im Bereich der Objekt-Segmentierung. Der Lern-Prozess konzentriert sich dabei meistens auf Bewegungen in Videodaten. Ein Beispiel stellt die Architektur aus \cite{DBLP:journals/corr/abs-1805-07780} dar, die Arcade-Spiele durch Reinforcement Learning lernt. Das Netz erhält zwei Frames als Eingabe und berechnet für jeden eine bestimmte Anzahl Objektmasken und entsprechende Translationen, sowie die Kamerabewegung. Beim Lern-Vorgang wird der Optische Fluss anhand der Ergebnisse ermittelt und eine Fehlerfunktion danach berechnet, wie genau der erste Frame aus dem zweitem und dem Optischen Fluss hervorgeht.\\
			Es existieren Ansätze zur Bildsegmentierung mit Neuronalen Netzen, die als schwach überwachtes Lernen bezeichnen werden können. Das Netz erhält dabei Bilder, von denen bekannt ist, ob sich ein bestimmtes Objekt darin befindet oder nicht. Die Idee ist, Objekte die in beiden Arten von Bildern vorkommen als Hintergrund zu erkennen und zu ignorieren. In \cite{10.1007/978-3-642-33863-2_20} wird ein Verfahren vorgestellt, dass mit großen Mengen zufällig ausgewählter, eventuell verrauschter Videos der Plattform YouTube arbeitet. Dabei wird ein Bild nach klassischen Segmentierungsverfahren in Regionen unterteilt. Die einzelnen Regionen werden von einem Netz danach bewertet, mit welcher Wahrscheinlichkeit sie zu dem gelernten Objekt gehören und anhand der Segmente mit hohen Wahrscheinlichkeiten wird, ebenfalls mit klassischen Methoden, eine Maske erstellt, die das gesamt Objekt umfassen soll.\\
			Wenn an ein Verfahren hohe Anforderungen gestellt werden, wie bei der panoptischen oder Instanz-Segmentierung ist eine überwachte Lern-Methode oft unerlässlich. Für jedes Bild eines Trainingssatzes muss dafür eine Ground Truth zur Verfügung stehen, die das gewünschte Ergebnis des Netzes darstellt. Für Beispiele für diese Art von Verfahren siehe Abschnitt \ref{sec:verArb}.
			
	\section{Arbeiten über Segmentierung durch überwachtes Lernen} \label{sec:verArb}
		In diesem Abschnitt wird auf vorhandene Arbeiten eingegangen, die sich mit der Problematik der Segmentierung von Bildern oder Punktwolken mit neuronalen Netzen befassen. 
		\subsection{PointNet}
			Das 2017 in \cite{PointNet} vorgestellte PointNet ist ein neuronales Netzwerk zum Auswerten von Punktwolken. Das Netz bietet dabei sowohl eine Architektur zur Klassifizierung als auch eine zur Segmentierung. Der Strukturelle Aufbau ist in Abbildung \ref{fig:pointnet} dargestellt. 
			\begin{figure}
				\centering
				\includegraphics[width=15cm]{img/PointNet.png}
				\caption{Architektur von PointNet nach \cite{PointNet}. Aus der Eingabe, die aus einer im Allgemeinen ungeordneten Menge an Punkten besteht, wird durch eine Reihe von Transformationen mittels Neuronaler Netze eine globale Signatur erstellt. Zu Klassifizierungsaufgaben kann dieser globale Feature-Vektor von einem weiteren Netz direkt weiterverarbeitet werden. Um eine Segmentierung durchzuführen, wird aus den lokalen und globalen Informationen ein neuer Vektor gebildet, der dann von einem Netz ausgewertet werden kann.}
				\label{fig:pointnet}
			\end{figure} 
			Problematisch an der Auswertung von Punktwolken mit Technologien der Neuroinformatik ist vor allem, dass die Daten im Allgemeinen ungeordnet sind. Das Netzwerk erzeugt darum aus einem Eingabevektor, der aus einer Menge von Koordinaten gebildet wird zunächst durch Feature Transformation eine globale Signatur, also einen Feature-Vektor, der unabhängig von der Reihenfolge der Eingabegrößen ist. PointNet erreicht dies durch den Einstatz von Max-Pooling. Um Invarianz bezüglich bestimmter räumlicher Transformationen wie z.B. Rotation zu erreichen, wird bei der Erstellung dieses globalen Feature-Vektor mit einem kleinen neuronalen Netz, dem "`T-Net"', eine Transformationsmatrix angenähert und auf die Eingabedaten angewandt. Mit der so berechneten Signatur kann ein weiteres Netz, in diesem Fall ein MLP, trainiert werden, das diese klassifiziert. Da der globale Feature-Vektor keine Ortsinformationen enthält, ist es nicht möglich, damit eine Segmentierung durchzuführen. Soll das Netzwerk also für diesen Zweck verwendet werden, wird der globale Feature-Vektor mit dem Eingabevektor kombiniert, um einen Vektor zu erzeugen, der sowohl globale als auch lokale Eigenschaften repräsentiert. Anschließend kann ein Label für jeden Punkt geschätzt werden.
			
		\subsection{UPSNet}
			Das 2019 in \cite{UPSNet} vorgestellte UPSNet (Unified Panoptic Segmentation Network) ist ein neuronales Netz für panoptische Segmentierung. Dazu führt das Netzwerk parallel eine semantische Segmentierung und eine Instanzsegmentierung des Eingabebildes durch und erstellte mit den kombinierten Ausgaben beider Methoden einen Tensor von Wahrscheinlichkeiten für jede Klasse und Instanz. Aus diesem Tensor wird in einem letzten Schritt ein Ausgabebild erzeugt. Der Aufbau des Netzwerks ist in Abbildung \ref{fig:upsarc} dargestellt.\\
			\begin{figure}
				\centering
				\includegraphics[width=15cm]{img/UPSNet.png}
				\caption{Architektur von UPSNet nach \cite{UPSNet}. Ein Mask R-CNN fungiert als Backbone des Netzes. Es erstellt einen Feature Vektor aufgrund der Eingabe, der dann von zwei leichtgewichtigen Netzen, dem "Semantic Head" und dem "Instance Head" verarbeitet wird. Die so entstandenen Ausgaben werden kombiniert und in dem "Panoptic Head" anhand einer Heuristik ausgewertet.}
				\label{fig:upsarc}
			\end{figure} 
			UPSNet verwendet als Backbone das in \cite{MaskRCNN} beschriebene Mask R-CNN, das sich aus dem ResNet und dem Feature Pyramid Network \cite{FPN} ableitet. Mask R-CNN führt eine Instanz-Segmentierung des Bildes durch und erzeugt parallel dazu eine Maske für jedes erkannte Objekt. Die Ausgabe des Backbones wird von zwei leichtgewichtigen Netzen, dem "`Semantic Segmentation Head"', der semantisch segmentiert und dem "`Instance Segmentation Head"', der eine Instanzsegmentierung durchführt unabhängig voneinander weiterverarbeitet. Die Implementierung eines einzelnen Backbones spart Rechenzeit und Speicherplatz gegenüber Architekturen mit zwei getrennten Netzen. Die daraus entstandenen Ergebnisse werden von dem "`Panoptic Segmentation Head"' anhand einer Heuristik ausgewertet, um die Netzwerkausgabe zu erstellen.\\
			Der "`Instance Segmentation Head"' folgt dem Konzept von Mask R-CNN und erzeugt eine Anzahl von Bounding Boxes und Masken. Der "`Semantic Segmentation Head"' ist ein CNN, das einen vom Backbone erzeugten Feature-Vektor als Eingabe erhält und mittels Soft-Max die Klasse jedes Pixels schätzt. Der "`Panoptic Segmentation Head"' ermittelt zuerst die Anzahl von im Bild vorhandener Instanzen und erstellt einen Tensor aus den von den beiden vorherigen Köpfen errechneten Wahrscheinlichkeiten und führt eine Soft-Max Berechnung durch. Aus dem Resultat wird anhand einer Heuristik für jeden Pixel entschieden, ob er einer Instanz eines zählbaren Objekts und welcher Klasse er angehört. Es ist dabei möglich, dass Pixel als unbekannt klassifiziert werden, was den IoU der Ergebnisse durch die Verminderung von False Positives verbessert.
			
		\subsection{DeepLab}
			Das in \cite{DeepLab1} und \cite{DeepLab2} vorgestellte DeepLab ist ein Framework zur semantischen Bildsegmentierung, das auf Deep Convolutional Neural Networks und Fully Connected Conditional Random Fields basiert. Die derzeit aktuelle Version ist DeepLabV3+ \cite{DBLP:journals/corr/abs-1802-02611}. Für weitere Informationen siehe Abschnitt \ref{sec:dl}.
			
	\section{Projektive Geometrie}
			Projektive Geometrie entstammt, wie in \cite{doi:10.1142/9789812384737_0010} erklärt, der Geometrie von Perspektive, die erstmals im 16. Jahrhundert von Malern der italienischen Renaissance untersucht wurde und der Photogrammetrie, die im Zuge der Entdeckung der Fotografie im 19. Jahrhundert entstand. Die Forschung in diesem Bereich gilt seit dem Beginn des 20. Jahrhunderts als abgeschlossen.